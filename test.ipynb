{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\3rd year\\Internship\\portofolios\\Video_predictor\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "from torchvision import models, transforms\n",
    "from torchvision.io import read_video\n",
    "from transformers import AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextFeatureExtractor, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class VideoFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VideoFeatureExtractor, self).__init__()\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Identity()  # Remove classification head\n",
    "\n",
    "    def forward(self, video):\n",
    "        batch_features = []\n",
    "        for batch_video in video: \n",
    "            video_features = []\n",
    "            for frame in batch_video:  # Loop through frames in a single video\n",
    "                frame = frame.unsqueeze(0)  # Add batch dimension for ResNet\n",
    "                feature = self.resnet(frame)  # Extract features for a single frame\n",
    "                video_features.append(feature.squeeze(0))  # Remove the batch dimension\n",
    "            video_features = torch.stack(video_features)  # Stack features for all frames\n",
    "            batch_features.append(video_features.mean(dim=0))  # Average across frames\n",
    "        return torch.stack(batch_features)  # Stack all video features in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, text_model_name, video_model, num_classes):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        self.video_model = video_model\n",
    "        self.fc = nn.Linear(768 + 2048, num_classes)  # Example sizes: 768 for text (BERT), 2048 for video features\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, video):\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.last_hidden_state.mean(dim=1)  # Shape: [batch_size, 768]\n",
    "        video_features = self.video_model(video)  # Shape: [batch_size, 2048]\n",
    "        combined_features = torch.cat((text_features, video_features), dim=-1)  # Shape: [batch_size, 2816]\n",
    "        return self.fc(combined_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the dataset for video-text pairs\n",
    "def safe_read_video(word_path):\n",
    "    try:\n",
    "        video, _, _ = read_video(word_path, pts_unit='sec')\n",
    "        return video\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading video {word_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GestureDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, tokenizer, max_length=128):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.categories = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "        self.category_word_map = {}\n",
    "        for category in self.categories:\n",
    "            category_path = os.path.join(dataset_dir, category)\n",
    "            words = [w for w in os.listdir(category_path) if os.path.isdir(os.path.join(category_path, w))]\n",
    "            self.category_word_map[category] = words\n",
    "        # print(f\"Categories and their words: {self.category_word_map}\")\n",
    "        self.samples = []\n",
    "        for category, words in self.category_word_map.items():\n",
    "            category_path = os.path.join(dataset_dir, category)\n",
    "            for word in words:\n",
    "                word_path = os.path.join(category_path, word)\n",
    "                videos = [f for f in os.listdir(word_path) if f.endswith(('.mp4', '.mov'))]\n",
    "                for video in videos:\n",
    "                    self.samples.append((category, word, os.path.join(word_path, video)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        category, word, video_path = self.samples[idx]\n",
    "        encoding = self.tokenizer(word, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        video, _, _ = read_video(video_path, pts_unit='sec')\n",
    "        if video.size(0) == 0:\n",
    "            print(f\"Warning: Video at {video_path} is empty or could not be read.\")\n",
    "            raise ValueError(f\"Invalid video file: {video_path}\")\n",
    "        transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "        video_transformed = []\n",
    "        for frame in video:\n",
    "            if frame.size(-1) != 3:\n",
    "                print(f\"Skipping frame with unexpected channels: {frame.size(-1)}\")\n",
    "                continue\n",
    "            frame = frame.permute(2, 0, 1)  # Convert to (C, H, W)\n",
    "            frame = transforms.ToPILImage()(frame)\n",
    "            video_transformed.append(transform(frame))\n",
    "        if len(video_transformed) == 0:\n",
    "            print(f\"Warning: No valid frames found in video: {video_path}\")\n",
    "            raise ValueError(f\"Video {video_path} has no valid frames.\")\n",
    "        target_length = 32\n",
    "        if len(video_transformed) > target_length:\n",
    "            video_transformed = video_transformed[:target_length]  \n",
    "        elif len(video_transformed) < target_length:\n",
    "            padding = target_length - len(video_transformed)\n",
    "            video_transformed += [torch.zeros((3, 224, 224))] * padding \n",
    "        video = torch.stack(video_transformed)\n",
    "        category_label = self.categories.index(category)\n",
    "        word_label = self.category_word_map[category].index(word)\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'video': video,\n",
    "            'category_labels': torch.tensor(category_label),\n",
    "            'word_labels': torch.tensor(word_label)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=1):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            video = batch['video']\n",
    "            category_labels = batch['category_labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask, video=video)\n",
    "            loss = criterion(output, category_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "train_dataset = GestureDataset(dataset_dir='processed_dataset/train', tokenizer=tokenizer)\n",
    "val_dataset = GestureDataset(dataset_dir='processed_dataset/val', tokenizer=tokenizer)\n",
    "test_dataset = GestureDataset(dataset_dir='processed_dataset/test', tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_model = \"distilbert-base-uncased\"\n",
    "video_model = VideoFeatureExtractor()\n",
    "# multi_modal_model = MultiModalModel(text_model, video_model, num_classes=16)\n",
    "multi_modal_model = MultiModalModel(\"distilbert-base-uncased\", video_model, num_classes=16)\n",
    "# train_model(multi_modal_model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_model(multi_modal_model, train_loader, val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
